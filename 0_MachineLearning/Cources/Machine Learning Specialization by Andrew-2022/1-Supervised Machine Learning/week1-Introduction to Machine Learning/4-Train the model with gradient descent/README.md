# Gradient descent
Start with w=0, b = 0
Keep changing w and b to reduce cost function J(w,b)

## Gradient descent intuition


## Learning rate

## Gradient descent for linear regression
- Linear regression model
- Cost function
- Gradient descent algorithm


Informally, a convex function is of bowl-shaped function and it cannot have any local minima other than the single global minimum. When you implement gradient descent on a convex function, one nice property is that so long as you're learning rate is chosen appropriately, it will always converge to the global minimum.
