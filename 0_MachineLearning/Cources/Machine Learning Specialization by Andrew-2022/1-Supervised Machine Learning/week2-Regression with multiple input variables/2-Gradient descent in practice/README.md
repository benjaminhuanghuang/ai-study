
## Feature scaling
Feature scaling that will enable gradient descent to `run much faster`

Implement feature scaling, to take features that take on very `different ranges` of values and skill them to have `comparable ranges` of values to each other

When features that take on very different ranges of values, scale them to have comparable ranges of values to each other

- Divide the Max value
- Mean normalization
```
    x = (x - mean) / (max - min)
```
- Z-score normalization
```
    x = (x - mean) / std
```

## Checking gradient descent for convergence


## Choosing the learning rate
Since the cost function is increasing, we know that gradient descent is diverging, so we need a lower learning rate. 


## Feature engineering
Feature engineering: use your knowledge or intuition about the problem to `design new features` usually by transforming or combining the original features of the problem in order to make it easier for the learning algorithm to make accurate predictions


## Polynomial regression
